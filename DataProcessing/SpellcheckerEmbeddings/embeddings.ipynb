{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m      2\u001b[0m embedding_cache_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/lucasvilsen/Desktop/GrammatikTAK/Datasets/dictionary_with_embeddings.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m embedding_cache \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43membedding_cache_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "embedding_cache_path = \"/Users/lucasvilsen/Desktop/GrammatikTAK/Datasets/dictionary_with_embeddings.csv\"\n",
    "embedding_cache = pickle.load(open(embedding_cache_path, \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai_client = openai.OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_BATCH_SIZE = 2048\n",
    "\n",
    "def get_embeddings(list_of_text: list[str], model=\"text-embedding-ada-002\", **kwargs) -> list[list[float]]:\n",
    "    assert len(list_of_text) <= 2048, \"The batch size should not be larger than 2048.\"\n",
    "\n",
    "    # replace newlines, which can negatively affect performance.\n",
    "    list_of_text = [text.replace(\"\\n\", \" \") for text in list_of_text]\n",
    "\n",
    "    data = openai.embeddings.create(input=list_of_text, model=model, **kwargs).data\n",
    "    return [d.embedding for d in data]\n",
    "\n",
    "def save_embeddings(embedding_cache, embedding_cache_path):\n",
    "    with open(embedding_cache_path, \"wb\") as embedding_cache_file:\n",
    "            pickle.dump(embedding_cache, embedding_cache_file)\n",
    "\n",
    "def update_embedding_cache(words, embeddings, embedding_cache):\n",
    "    for word, embedding in zip(words, embeddings):\n",
    "        embedding_cache[word] = embedding\n",
    "    return embedding_cache\n",
    "\n",
    "def run_batch(words, embedding_cache, embedding_cache_path):\n",
    "    batch_words = get_words_without_embeddings(words, embedding_cache)[:MAX_BATCH_SIZE]\n",
    "    batch_embeddings = get_embeddings(batch_words)\n",
    "    embedding_cache = update_embedding_cache(batch_words, batch_embeddings, embedding_cache)\n",
    "    save_embeddings(embedding_cache, embedding_cache_path)\n",
    "    words_remaining = len(get_words_without_embeddings(words, embedding_cache))\n",
    "    if words_remaining > 0: print(f\"Words to embed remaining: {words_remaining}\"); return True \n",
    "    return False\n",
    "\n",
    "def get_words_without_embeddings(words, embedding_cache):\n",
    "    lst_embed_cache = list(embedding_cache.keys())\n",
    "    return [v for v in words if v not in lst_embed_cache]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all embeddings:\n",
    "embedding_cache_path = \"/Users/lucasvilsen/Desktop/GrammatikTAK/Datasets/dictionary_with_embeddings.pickle\"\n",
    "words = sorted(pickle.load(open(\"/Users/lucasvilsen/Desktop/GrammatikTAK/Datasets/dictionary.pickle\", \"rb\")))\n",
    "embedding_cache = pickle.load(open(embedding_cache_path, \"rb\"))\n",
    "batches_to_run = (len(words) - len(embedding_cache)) // 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m to_continue \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m to_continue:\n\u001b[0;32m----> 3\u001b[0m     to_continue \u001b[38;5;241m=\u001b[39m \u001b[43mrun_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_cache_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[47], line 22\u001b[0m, in \u001b[0;36mrun_batch\u001b[0;34m(words, embedding_cache, embedding_cache_path)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_batch\u001b[39m(words, embedding_cache, embedding_cache_path):\n\u001b[0;32m---> 22\u001b[0m     batch_words \u001b[38;5;241m=\u001b[39m \u001b[43mget_words_without_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_cache\u001b[49m\u001b[43m)\u001b[49m[:MAX_BATCH_SIZE]\n\u001b[1;32m     23\u001b[0m     batch_embeddings \u001b[38;5;241m=\u001b[39m get_embeddings(batch_words)\n\u001b[1;32m     24\u001b[0m     embedding_cache \u001b[38;5;241m=\u001b[39m update_embedding_cache(batch_words, batch_embeddings, embedding_cache)\n",
      "Cell \u001b[0;32mIn[47], line 32\u001b[0m, in \u001b[0;36mget_words_without_embeddings\u001b[0;34m(words, embedding_cache)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_words_without_embeddings\u001b[39m(words, embedding_cache):\n\u001b[1;32m     31\u001b[0m     lst_embed_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(embedding_cache\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwords\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlst_embed_cache\u001b[49m\u001b[43m]\u001b[49m\n",
      "Cell \u001b[0;32mIn[47], line 32\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_words_without_embeddings\u001b[39m(words, embedding_cache):\n\u001b[1;32m     31\u001b[0m     lst_embed_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(embedding_cache\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [v \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m lst_embed_cache]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "to_continue = True\n",
    "while to_continue:\n",
    "    to_continue = run_batch(words, embedding_cache, embedding_cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2010"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
