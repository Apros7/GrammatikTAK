{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "671"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"composite_words.csv\", sep=\"\\t\")\n",
    "always_true = df[df[\"forklaring\"] == \"altid sådan\"]\n",
    "composite_words = list(always_true[\"ord/ordforbindelse\"].values)\n",
    "composite_words = [word for word in composite_words if \"(\" not in word]\n",
    "# currently only correct 2-3 composite words in spellchecker\n",
    "composite_words = [word for word in composite_words if len(word.split()) < 4] \n",
    "len(composite_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2: 298, 1: 311, 3: 62})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter([len(word.split()) for word in composite_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word for word in composite_words if len(word.split()) > 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(311, 298, 62)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "composite_one = [word for word in composite_words if len(word.split()) == 1]\n",
    "composite_two = [word for word in composite_words if len(word.split()) == 2]\n",
    "composite_three = [word for word in composite_words if len(word.split()) == 3]\n",
    "len(composite_one), len(composite_two), len(composite_three)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['actionfilm', 'afstedkomme', 'ahaoplevelse', 'ajour', 'allerbedst'],\n",
       " ['aber dabei', 'a cappella', 'a cappella-kor', 'accent aigu', 'accent grave'],\n",
       " ['a la carte',\n",
       "  'a la carte-ret',\n",
       "  'a la grecque-bort',\n",
       "  'al den stund',\n",
       "  'alt i alt'])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "composite_one[:5], composite_two[:5], composite_three[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Composite word two\n",
      "adgangen\n",
      "afhænde\n",
      "billigbog\n",
      "desværre\n",
      "engang\n",
      "førstebehandling\n",
      "førsteklasses\n",
      "glatis\n",
      "godaften\n",
      "goddag\n",
      "godmorgen\n",
      "godnat\n",
      "iland\n",
      "lilleby\n",
      "lillejuleaften\n",
      "lilleskole\n",
      "overstyr\n",
      "småbitte\n",
      "stormagt\n",
      "tilkende\n",
      "tilstede\n",
      "udeomkring\n",
      "udfor\n",
      "velsagtens\n",
      "veltilfreds\n",
      "veltilpas\n",
      "vistnok\n",
      "Composite word three\n"
     ]
    }
   ],
   "source": [
    "dictionary = pickle.load(open(\"../../Datasets/dictionary.pickle\", \"rb\"))\n",
    "composite_dict = {}\n",
    "for comp in composite_one:\n",
    "    composite_dict[comp] = comp\n",
    "print(\"Composite word two\")\n",
    "for comp in composite_two:\n",
    "    if comp.replace(\" \", \"\") not in dictionary:\n",
    "        composite_dict[comp.replace(\" \", \"\")] = comp\n",
    "    else:\n",
    "        print(comp.replace(\" \", \"\"))\n",
    "print(\"Composite word three\")\n",
    "for comp in composite_three:\n",
    "    word1, word2, word3 = comp.split()\n",
    "    if word1 + word2 + \" \" + word3 not in dictionary:\n",
    "        composite_dict[word1 + word2 + \" \" + word3] = comp\n",
    "    else:\n",
    "        print(word1 + word2 + \" \" + word3)\n",
    "    if word1 + \" \" + word2 + word3 not in dictionary:\n",
    "        composite_dict[word1 + \" \" + word2 + word3] = comp\n",
    "    else:\n",
    "        print(word1 + \" \" + word2 + word3)\n",
    "    if word1 + word2 + word3 not in dictionary: \n",
    "        composite_dict[word1 + word2 + word3] = comp\n",
    "    else:\n",
    "        print(word1 + word2 + word3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "767"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(composite_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_features_to_dicts(features):\n",
    "    feature_dicts = []\n",
    "    current_tense = None\n",
    "    for feature in features:\n",
    "        if feature is None:\n",
    "            feature_dicts.append({})\n",
    "            continue\n",
    "        feature_dict = {}\n",
    "        current_features = feature.split(\"|\")\n",
    "        for current_feature in current_features:\n",
    "            key, value = current_feature.split(\"=\")\n",
    "            if key == \"Tense\" and current_tense is None:\n",
    "                current_tense = value\n",
    "            feature_dict[key] = value\n",
    "        if \"Tense\" not in feature_dict and \"VerbForm\" in feature_dict and key is not None:\n",
    "            feature_dict[\"Tense\"] = \"Pres\" if current_tense is None else current_tense\n",
    "        feature_dicts.append(feature_dict)\n",
    "    return feature_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-19 22:47:51 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5395f9b2fdf841fb86e67848783f78f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-19 22:47:52 INFO: Loading these models for language: da (Danish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ddt     |\n",
      "| pos       | ddt     |\n",
      "=======================\n",
      "\n",
      "2023-06-19 22:47:52 WARNING: GPU requested, but is not available!\n",
      "2023-06-19 22:47:52 INFO: Using device: cpu\n",
      "2023-06-19 22:47:52 INFO: Loading: tokenize\n",
      "2023-06-19 22:47:52 INFO: Loading: pos\n",
      "2023-06-19 22:47:52 INFO: Done loading processors!\n",
      "100%|██████████| 767/767 [00:25<00:00, 29.96it/s]\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "from tqdm import tqdm\n",
    "pos_model = stanza.Pipeline(\"da\", processors='tokenize,pos', use_gpu=True, cache_directory='./cache', tokenize_pretokenized=True, n_process=4)\n",
    "for k, v in tqdm(composite_dict.items()):\n",
    "    doc = pos_model(v)\n",
    "    features = [word.feats if word.feats else None for sentence in doc.sentences for word in sentence.words]\n",
    "    feature_dicts = turn_features_to_dicts(features)\n",
    "    results = [[word.upos, [], feature_dicts[i]] for sentence in doc.sentences for i, word in enumerate(sentence.words)]\n",
    "    composite_dict[k] = (v, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "767"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(composite_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"composite_dict.pickle\", \"wb\") as f:\n",
    "    pickle.dump(composite_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = pickle.load(open(\"../../Datasets/dictionary.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "spelling_errors = pickle.load(open(\"../../Datasets/misspellings_dict.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23732830"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spelling_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(spelling_errors.keys())\n",
    "values = list(spelling_errors.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"spelling_errors_keys.pickle\", \"wb\") as f:\n",
    "    pickle.dump(keys, f)\n",
    "with open(\"spelling_errors_values.pickle\", \"wb\") as f:\n",
    "    pickle.dump(values, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'spelling_errors_keys.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m keys \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspelling_errors_keys.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      2\u001b[0m values \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspelling_errors_values.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'spelling_errors_keys.pickle'"
     ]
    }
   ],
   "source": [
    "keys = pickle.load(open(\"spelling_errors_keys.pickle\", \"rb\"))\n",
    "values = pickle.load(open(\"spelling_errors_values.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "spelling_errors = {k: v for k,v in zip(keys, values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23732830"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spelling_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
