{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForCausalLM\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import lemmy\n",
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 23:37:13 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5023f9c1540d4fa2b87f58a9a4c7ed90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 23:37:14 INFO: Loading these models for language: da (Danish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ddt     |\n",
      "| pos       | ddt     |\n",
      "=======================\n",
      "\n",
      "2023-03-26 23:37:14 WARNING: GPU requested, but is not available!\n",
      "2023-03-26 23:37:14 INFO: Using device: cpu\n",
      "2023-03-26 23:37:14 INFO: Loading: tokenize\n",
      "2023-03-26 23:37:14 INFO: Loading: pos\n",
      "2023-03-26 23:37:14 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "pos_model = stanza.Pipeline(\"da\", processors='tokenize,pos', use_gpu=True, cache_directory='./cache', tokenize_pretokenized=True, n_process=4)\n",
    "lemmatizer = lemmy.load(\"da\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos(sentence):\n",
    "    doc = pos_model(sentence)\n",
    "    return [word.upos for sentence in doc.sentences for word in sentence.words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sentence(sentences):\n",
    "\n",
    "    for x in tqdm(range(0, len(sentences))):\n",
    "        cur_sentences = sentences[x]\n",
    "        cur_sentence = \" \".join(cur_sentences)\n",
    "        words = cur_sentence.split()\n",
    "        len_cur_sent = len(words)\n",
    "        pos = np.array(get_pos(cur_sentence))\n",
    "        verb_indices = np.where(pos == \"VERB\")[0]\n",
    "        if len(pos) != len(words):\n",
    "            continue\n",
    "        for i in verb_indices:\n",
    "            if i < 4:\n",
    "                words_after = [\"<pad>\"]*(4-i) + words[0:i+5]\n",
    "                words_before = [\"<pad>\"]*(4-i) + words[0:i] + [lemmatizer.lemmatize(pos[i], words[i])[0]] + words[i+1:i+5]\n",
    "                value = 1\n",
    "            elif i >= len_cur_sent-4:\n",
    "                words_after = words[i-4:] + [\"<pad>\"]*(4-(len_cur_sent-i)+1)\n",
    "                words_before = words[i-4:i] + [lemmatizer.lemmatize(pos[i], words[i])[0]] + words[i+1:] + [\"<pad>\"]*(4-(len_cur_sent-i)+1)\n",
    "                value = 2\n",
    "            else:\n",
    "                words_after = words[i-4:i+5]\n",
    "                words_before = words[i-4:i] + [lemmatizer.lemmatize(pos[i], words[i])[0]] + words[i+1:i+5]\n",
    "                current_sentence.append([(\" \".join(words_before)).lower(), (\" \".join(words_after)).lower()])\n",
    "                value = 3\n",
    "            if len(words_before) != 9:\n",
    "                print(\"Error:\")\n",
    "                print(words_before, value)\n",
    "                print(i, len_cur_sent)\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv('../GrammatiktakDatasets/otherDatasets/nutids_r.csv', names=[\"wrong\", \"right\"])\n",
    "test_lines = list(test_set[\"wrong\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
