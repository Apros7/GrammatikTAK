{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForCausalLM\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import lemmy\n",
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-27 14:30:36 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab82c8124cb94323b1db18bb3bad9300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-27 14:30:37 INFO: Loading these models for language: da (Danish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ddt     |\n",
      "| pos       | ddt     |\n",
      "=======================\n",
      "\n",
      "2023-03-27 14:30:37 WARNING: GPU requested, but is not available!\n",
      "2023-03-27 14:30:37 INFO: Using device: cpu\n",
      "2023-03-27 14:30:37 INFO: Loading: tokenize\n",
      "2023-03-27 14:30:37 INFO: Loading: pos\n",
      "2023-03-27 14:30:37 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "pos_model = stanza.Pipeline(\"da\", processors='tokenize,pos', use_gpu=True, cache_directory='./cache', tokenize_pretokenized=True, n_process=4)\n",
    "lemmatizer = lemmy.load(\"da\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos(sentence):\n",
    "    doc = pos_model(sentence)\n",
    "    return [word.upos for sentence in doc.sentences for word in sentence.words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sentence(sentences):\n",
    "    number_of_verbs = []\n",
    "    dataset = []\n",
    "    for x in tqdm(range(0, len(sentences))):\n",
    "        cur_sentences = sentences[x]\n",
    "        cur_sentence = \"\".join(cur_sentences)\n",
    "        words = cur_sentence.split()\n",
    "        len_cur_sent = len(words)\n",
    "        pos = np.array(get_pos(cur_sentence))\n",
    "        verb_indices = np.where(pos == \"VERB\")[0]\n",
    "        if len(pos) != len(words):\n",
    "            continue\n",
    "        for i in verb_indices:\n",
    "            already_padded = False\n",
    "            if i < 4:\n",
    "                words_before = [\"<pad>\"]*(4-i) + words[0:i] + [lemmatizer.lemmatize(pos[i], words[i])[0]] + words[i+1:i+5]\n",
    "                if i >= len_cur_sent-4:\n",
    "                    words_before = words_before + [\"<pad>\"]*(4-(len_cur_sent-i)+1)\n",
    "            elif i >= len_cur_sent-4:\n",
    "                words_before = words[i-4:i] + [lemmatizer.lemmatize(pos[i], words[i])[0]] + words[i+1:] + [\"<pad>\"]*(4-(len_cur_sent-i)+1)\n",
    "            else:\n",
    "                words_before = words[i-4:i] + [lemmatizer.lemmatize(pos[i], words[i])[0]] + words[i+1:i+5]\n",
    "            if len(words_before) != 9:\n",
    "                print(\"Error:\")\n",
    "                print(words_before, i)\n",
    "                continue\n",
    "            dataset.append((\" \".join(words_before)).lower())\n",
    "        number_of_verbs.append(len(verb_indices))\n",
    "    return dataset, number_of_verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 11.33it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['glæde sig til at se og inviterer familie og',\n",
       " 'til at ser og invitere familie og venner <pad>',\n",
       " 'ikke altid nemt at forsvare din opførsel <pad> <pad>',\n",
       " '<pad> <pad> mange drenge interessere sig for fodbold <pad>',\n",
       " '<pad> <pad> vil du invitere alle dine veninder til',\n",
       " 'det er svært at vurdere hvor meget bilen er']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = pd.read_csv('../GrammatiktakDatasets/otherDatasets/nutids_r.csv', names=[\"wrong\", \"right\"])\n",
    "test_lines = list(test_set[\"wrong\"])\n",
    "dataset, number_of_verbs = convert_sentence(test_lines[:5])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\"\n",
    "tenseModel = torch.load(\"tenseModel1\", map_location=torch.device('mps'))\n",
    "tenseModel.eval()\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X_tokenized, Y_tokenized=None):\n",
    "        self.input_ids = X_tokenized[\"input_ids\"]\n",
    "        self.attention_mask = X_tokenized[\"attention_mask\"]\n",
    "        if Y_tokenized is not None:\n",
    "            self.labels = Y_tokenized[\"input_ids\"]\n",
    "        else:\n",
    "            self.labels = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.input_ids[idx]\n",
    "        attention_mask = self.attention_mask[idx]\n",
    "        if self.labels is None:\n",
    "            return torch.tensor(input_ids), torch.tensor(attention_mask)\n",
    "        labels = self.labels[idx]\n",
    "        return torch.tensor(input_ids), torch.tensor(attention_mask), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"DDSC/roberta-base-danish\")\n",
    "\n",
    "X_train_tokenized = tokenizer(dataset, padding=True, truncation=True)\n",
    "#train_dataset = CustomDataset(X_train_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['glæde sig til at se og familie familie og', 'til at invitere familie og venner', 'ikke, fint at min', 'mange drengesere sig', 'vil jeg alle til', 'det er svært at vurdere hvor meget bilen er']\n"
     ]
    }
   ],
   "source": [
    "input_ids = X_train_tokenized['input_ids']\n",
    "attention_mask = X_train_tokenized['attention_mask']\n",
    "\n",
    "input_ids = torch.tensor(input_ids)\n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = tenseModel(input_ids.to(device), attention_mask=attention_mask.to(device))\n",
    "\n",
    "# Get the predicted labels\n",
    "predicted_labels = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "# Convert the predicted labels to text using the tokenizer\n",
    "predicted_texts = [tokenizer.decode(ids, skip_special_tokens=True) for ids in predicted_labels]\n",
    "\n",
    "print(predicted_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['logits', 'logits', 'logits', 'logits', 'logits', 'logits', 'logits']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
