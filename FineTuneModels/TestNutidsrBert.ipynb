{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from transformers import Trainer, BertTokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/lucasvilsen/Desktop/GrammatikTAK/GrammatiktakDatasets/otherDatasets/\")\n",
    "test_sentences = pd.read_csv(\"nutids_r.csv\", header=None, names=[\"fake\", \"true\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "os.chdir(\"/Users/lucasvilsen/Desktop/GrammatikTAK/Datasets/\")\n",
    "with open(\"nutids_r.pickle\", \"rb\") as f:\n",
    "    nutids_r = pickle.load(f)\n",
    "with open(\"nutids_r_stem.pickle\", \"rb\") as f:\n",
    "    nutids_r_stem = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rigtig mange glæde sig til at ser og inviterer familie og venner',\n",
       " 'det er ikke altid nemt at forsvarer din opførsel',\n",
       " 'mange drenge interessere sig for fodbold',\n",
       " 'vil du inviterer alle dine veninder til fødselsdagen',\n",
       " 'det er svært at vurderer hvor meget bilen er værd']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [x.strip().strip(\".,?!\").lower() for x in list(test_sentences[\"fake\"])]\n",
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
    "import stanza\n",
    "\n",
    "char = \"*@;:!\\\"?«».,\"\n",
    "\n",
    "def init_model():\n",
    "    model = torch.load(\"nutidsrModel1\", map_location=torch.device('cpu'))\n",
    "    device = \"mps\"\n",
    "    torch.device(device)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    return Trainer(model)\n",
    "\n",
    "def load_constants():\n",
    "    SCOPE = 10\n",
    "    PADDING = int(SCOPE/2)\n",
    "    HALF_SCOPE = int(SCOPE/2)\n",
    "    MAX_LENGTH = 21\n",
    "    return SCOPE, PADDING, HALF_SCOPE, MAX_LENGTH\n",
    "\n",
    "class correct_nutidsr():\n",
    "    def __init__(self):\n",
    "        os.chdir(\"/Users/lucasvilsen/Desktop/GrammatikTAK/FineTuneModels/Models/\")\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('Maltehb/danish-bert-botxo')\n",
    "        self.model = init_model()\n",
    "        self.pos_tagger = stanza.Pipeline(\"da\", processors='tokenize,pos', use_gpu=True, cache_directory='./cache', tokenize_pretokenized=True, n_process=4)\n",
    "        self.scope, self.padding, self.half_scope, self.max_length = load_constants()\n",
    "\n",
    "    def get_pos_tags(self, sentence):\n",
    "        doc = self.pos_tagger(sentence)\n",
    "        results = [word.upos for sentence in doc.sentences for word in sentence.words]\n",
    "        return results\n",
    "    \n",
    "    def add_padding(self, lsts):\n",
    "        return [[\"<PAD>\"]*self.padding + lst + [\"<PAD>\"]*self.padding for lst in lsts]\n",
    "    \n",
    "    def clean_input(self, pre_cleaned_result):\n",
    "        post_cleaned_result = []\n",
    "        for i in tqdm(range(len(pre_cleaned_result))):\n",
    "            pre_cleaned_lst = pre_cleaned_result[i]\n",
    "            lst = [word.translate(str.maketrans('', '', ''.join(char))) for word in pre_cleaned_lst]\n",
    "            post_cleaned_result.append(lst)\n",
    "        return post_cleaned_result\n",
    "    \n",
    "    def convert_sentences_to_dataset(self, post_cleaned_result):\n",
    "        post_cleaned_result\n",
    "        dataset = []\n",
    "        original_words = []\n",
    "        output_lst = []\n",
    "        for i in tqdm(range(len(post_cleaned_result))):\n",
    "            cur_original_words = []\n",
    "            current_lst = post_cleaned_result[i]\n",
    "            pos_tags = self.get_pos_tags(\" \".join(current_lst))\n",
    "            for x in range(len(current_lst)):\n",
    "                current_word = current_lst[x]\n",
    "                try: stemmed_word = nutids_r_stem[current_word]\n",
    "                except: continue\n",
    "                if pos_tags[x] != \"VERB\":\n",
    "                    continue\n",
    "                current_dataset = current_lst[x-self.half_scope:x+self.half_scope+1]\n",
    "                current_dataset[self.half_scope] = stemmed_word\n",
    "                dataset.append(\" \".join(current_dataset))\n",
    "                if nutids_r[stemmed_word][0] == current_word:\n",
    "                    output = 1\n",
    "                elif nutids_r[stemmed_word][1] == current_word:\n",
    "                    output = 0\n",
    "                else:\n",
    "                    print(\"ERROR\")\n",
    "                output_lst.append(output)\n",
    "                cur_original_words.append(current_word)\n",
    "            original_words.append(cur_original_words)\n",
    "        return dataset, output_lst, original_words\n",
    "\n",
    "    def convert_dataset_to_dataloader(self, dataset):\n",
    "        test_dataset = Dataset(dataset)\n",
    "        return test_dataset\n",
    "    \n",
    "    def tokenize_sentences(self, sentences):\n",
    "        X_tokenized = self.tokenizer(sentences, padding=True, truncation=True, max_length=self.max_length)\n",
    "        return X_tokenized\n",
    "    \n",
    "    def get_predictions(self, dataset):\n",
    "        raw_predictions, _, _ = self.model.predict(dataset)\n",
    "        final_prediction = np.argmax(raw_predictions, axis=1)\n",
    "        return final_prediction\n",
    "\n",
    "    def from_sentences_to_dataset(self, sentences):\n",
    "        padded_sentences = self.add_padding(sentences)\n",
    "        dataset, output_lst, original_words = self.convert_sentences_to_dataset(padded_sentences)\n",
    "        X_tokenized = self.tokenize_sentences(dataset)\n",
    "        test_dataset = self.convert_dataset_to_dataloader(X_tokenized)\n",
    "        return test_dataset, output_lst, original_words, dataset\n",
    "    \n",
    "    def get_measures(self, predictions, output_lst, dataset):\n",
    "        middle_words = [data.split()[self.half_scope] for data in dataset]\n",
    "        df = pd.DataFrame({\n",
    "                            \"Dataset\": dataset,\n",
    "                            \"Preds\": predictions,\n",
    "                            \"Output\": output_lst,\n",
    "                            \"Word\": middle_words\n",
    "                        })\n",
    "        os.chdir(\"/Users/lucasvilsen/Desktop/GrammatikTAK/GrammatiktakDatasets/otherDatasets/\")\n",
    "        df.to_csv(\"nutidsr_test_results.csv\", index=False)\n",
    "        f1 = f1_score(output_lst, predictions, average=\"macro\")\n",
    "        recall = recall_score(output_lst, predictions, average=\"macro\")\n",
    "        precision = precision_score(output_lst, predictions, average=\"macro\")\n",
    "        accuracy = accuracy_score(output_lst, predictions)\n",
    "        return [f1, recall, precision, accuracy]\n",
    "    \n",
    "    def print_measures(self, measures, measure_names):\n",
    "        for measure, name in zip(measures, measure_names):\n",
    "            print(f\"{name}: {round(measure, 4)}\")\n",
    "\n",
    "    def split_sentences(self, sentences):\n",
    "        return [sent.split() for sent in sentences]\n",
    "\n",
    "    def correct_sentence(self, test_sentences):\n",
    "        sentences = self.split_sentences(test_sentences)\n",
    "        dataset, output, original_words, pre_tokenized_dataset = self.from_sentences_to_dataset(sentences)\n",
    "        predictions = self.get_predictions(dataset)\n",
    "        self.print_measures(self.get_measures(predictions, output, pre_tokenized_dataset), [\"F1-score\", \"Recall\", \"Precision\", \"Accuracy\"])\n",
    "        self.convert_back_to_sentences(test_sentences, predictions, original_words)\n",
    "        \n",
    "    def convert_back_to_sentences(self, sentences, predictions, original_words_lst):\n",
    "        true_sentences = [x.strip().strip(\".,?!\").lower() for x in list(test_sentences[\"true\"])]\n",
    "        i = 0\n",
    "        correct = 0\n",
    "        for sentence, true_sentence, original_words in zip(sentences, true_sentences, original_words_lst):\n",
    "            replacements = []\n",
    "            for original_word in original_words:\n",
    "                stemmed_word = nutids_r_stem[original_word]\n",
    "                if predictions[i] == 0:\n",
    "                    correct_word = nutids_r[stemmed_word][0]\n",
    "                else:\n",
    "                    correct_word = nutids_r[stemmed_word][1]\n",
    "                sentence = sentence.replace(original_word, correct_word)\n",
    "                replacements.append(f\"{original_word} -> {correct_word}\")\n",
    "                i += 1\n",
    "            # print(sentence)\n",
    "            # print(*replacements, sep=\"\\n\")\n",
    "            # print(\"\\n\")\n",
    "            if sentence == true_sentence:\n",
    "                correct += 1\n",
    "            else:\n",
    "                print(sentence)\n",
    "                print(true_sentence)\n",
    "\n",
    "        print(f\"Correct: {correct}/{len(true_sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-10 14:57:43 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c425310f1c445e0ab8db64c836531fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-10 14:57:44 INFO: Loading these models for language: da (Danish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ddt     |\n",
      "| pos       | ddt     |\n",
      "=======================\n",
      "\n",
      "2023-04-10 14:57:44 WARNING: GPU requested, but is not available!\n",
      "2023-04-10 14:57:44 INFO: Using device: cpu\n",
      "2023-04-10 14:57:44 INFO: Loading: tokenize\n",
      "2023-04-10 14:57:44 INFO: Loading: pos\n",
      "2023-04-10 14:57:44 INFO: Done loading processors!\n",
      "100%|██████████| 100/100 [00:14<00:00,  6.95it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0b8e6a4b26845e7b288a4ae7c2a94d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.8581\n",
      "Recall: 0.8869\n",
      "Precision: 0.8348\n",
      "Accuracy: 0.9406\n",
      "rigtig mange glæde sig til at ser og invitere familie og venner\n",
      "rigtig mange glæder sig til at se og invitere familie og venner\n",
      "de funden en løsning\n",
      "de finder en løsning\n",
      "jeg skreven en e-mail til min chef\n",
      "jeg skriver en e-mail til min chef\n",
      "han spille fodbold hver weekend\n",
      "han spiller fodbold hver weekend\n",
      "hun malt et billede til sin mor\n",
      "hun maler et billede til sin mor\n",
      "vi tagen bussen til skolen\n",
      "vi tager bussen til skolen\n",
      "han fløjen til paris i morgen\n",
      "han flyver til paris i morgen\n",
      "du given mig en gave på min fødselsdag\n",
      "du giver mig en gave på min fødselsdag\n",
      "han tagen elevatoren op til kontoret\n",
      "han tager elevatoren op til kontoret\n",
      "de sås en film i biografen\n",
      "de ser en film i biografen\n",
      "vi spille brætspil om aftenen\n",
      "vi spiller brætspil om aftenen\n",
      "du skreven en liste over tingene\n",
      "du skriver en liste over tingene\n",
      "du boende i en stor by\n",
      "du bor i en stor by\n",
      "jeg skreven en dagbog hver aften\n",
      "jeg skriver en dagbog hver aften\n",
      "han sås en basketballkamp i fjernsynet\n",
      "han ser en basketballkamp i fjernsynet\n",
      "du spille computer hver aften\n",
      "du spiller computer hver aften\n",
      "hun tagen en tur i biografen med sine venner\n",
      "hun tager en tur i biografen med sine venner\n",
      "du sås solnedgangen på stranden\n",
      "du ser solnedgangen på stranden\n",
      "de tagen billeder af seværdighederne på ferien\n",
      "de tager billeder af seværdighederne på ferien\n",
      "han spille guitar i et band om aftenen\n",
      "han spiller guitar i et band om aftenen\n",
      "de sås en teaterforestilling i weekenden\n",
      "de ser en teaterforestilling i weekenden\n",
      "jeg kan godt lide at gø det\n",
      "jeg kan godt lide at gøre det\n",
      "jeg gørende det bare\n",
      "jeg gør det bare\n",
      "han sås en god film på netflix i går aftes\n",
      "han ser en god film på netflix i går aftes\n",
      "de malt deres hus i sommeren sidste år\n",
      "de maler deres hus i sommeren sidste år\n",
      "hun bagt en lækker kage til sin fødselsdag\n",
      "hun bager en lækker kage til sin fødselsdag\n",
      "han lærer at taler spansk som voksen\n",
      "han lærer at tale spansk som voksen\n",
      "han spille fodbold med sine venner hver uge\n",
      "han spiller fodbold med sine venner hver uge\n",
      "hun malt smukke billeder i sin fritid\n",
      "hun maler smukke billeder i sin fritid\n",
      "jeg sås en dokumentarfilm om naturen i aften\n",
      "jeg ser en dokumentarfilm om naturen i aften\n",
      "de spille brætspil sammen hver søndag\n",
      "de spiller brætspil sammen hver søndag\n",
      "jeg tagen en tur til stranden for at nyde solen\n",
      "jeg tager en tur til stranden for at nyde solen\n",
      "Correct: 68/100\n"
     ]
    }
   ],
   "source": [
    "corrector = correct_nutidsr() \n",
    "corrector.correct_sentence(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
