{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from transformers import Trainer, BertTokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/lucasvilsen/Desktop/GrammatikTAK/\")\n",
    "test_sentences_verbs = pd.read_csv(\"Datasets/EuroparlNutidsr_testset.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from GrammatiktakBackend.Utilities.utils import find_index, prepare_sentence\n",
    "from transformers import pipeline\n",
    "import tqdm\n",
    "tqdm.disable = True\n",
    "\n",
    "def load_model(path):\n",
    "    device = \"mps\"\n",
    "    torch.device(device)\n",
    "    classifier = torch.load(path, map_location=torch.device('cpu'))\n",
    "    classifier.eval()\n",
    "    classifier.to(device)\n",
    "    return Trainer(classifier)\n",
    "\n",
    "class NutidsRCorrector():\n",
    "    def __init__(self, model, pos, padding):\n",
    "        self.can_verb_be_checked = pickle.load(open(\"Datasets/nutids_r_stem.pickle\", \"rb\"))\n",
    "        self.get_tense_from_verb = pickle.load(open(\"Datasets/nutids_r_bøjninger.pickle\", \"rb\"))\n",
    "        self.classifier = model\n",
    "        self.padding = padding\n",
    "        self.pos = pos\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('Maltehb/danish-bert-botxo')\n",
    "\n",
    "    def should_be_nutidsr(self, verbs_to_check):\n",
    "        dataset = self.make_dataset(verbs_to_check)\n",
    "        if len(dataset) < 1:\n",
    "            return [None]*len(verbs_to_check)\n",
    "        tokenized = self.tokenize_sentences(dataset)\n",
    "        dataloader = self.convert_dataset_to_dataloader(tokenized)\n",
    "        predictions = self.get_predictions(dataloader)\n",
    "        return list(self.turn_predictions_to_bool(predictions, verbs_to_check))\n",
    "\n",
    "    def turn_predictions_to_bool(self, predictions, verbs_to_check):\n",
    "        prediction_index = 0\n",
    "        for i in range(len(verbs_to_check)):\n",
    "            if verbs_to_check[i]:\n",
    "                if predictions[prediction_index] == 0:\n",
    "                    yield True\n",
    "                yield False\n",
    "                prediction_index += 1\n",
    "            else:\n",
    "                yield None\n",
    "\n",
    "    def convert_dataset_to_dataloader(self, dataset):\n",
    "        test_dataset = Dataset(dataset)\n",
    "        return test_dataset\n",
    "    \n",
    "    def tokenize_sentences(self, sentences):\n",
    "        X_tokenized = self.tokenizer(sentences, padding=True, truncation=True, max_length=26)\n",
    "        return X_tokenized\n",
    "    \n",
    "    def get_predictions(self, dataloader):\n",
    "        raw_predictions, _, _ = self.classifier.predict(dataloader)\n",
    "        final_prediction = np.argmax(raw_predictions, axis=1)\n",
    "        return final_prediction\n",
    "\n",
    "    def make_dataset(self, verbs_to_check):\n",
    "        pos_with_padding = [\"<PAD>\"]*self.padding + [p[0] for p in self.pos] + [\"<PAD>\"]*self.padding\n",
    "        dataset = []\n",
    "        for i in range(len(verbs_to_check)):\n",
    "            if verbs_to_check[i]:\n",
    "                dataset.append(\" \".join(pos_with_padding[i:i+2*self.padding+1]))\n",
    "        return dataset\n",
    "\n",
    "    def verbs_to_check(self, words):\n",
    "        pos = self.pos\n",
    "        verbs = []\n",
    "        for i in range(len(pos)):\n",
    "            if pos[i][0] != \"VERB\":\n",
    "                verbs.append(False)\n",
    "            elif \"Tense\" not in pos[i][2].keys():\n",
    "                verbs.append(False)\n",
    "            elif pos[i][2][\"Tense\"] != \"Pres\":\n",
    "                verbs.append(False)\n",
    "            else:\n",
    "                verbs.append(True)\n",
    "        for i, bool in enumerate(verbs):\n",
    "            if not bool:\n",
    "                continue\n",
    "            word = words[i].strip(\",.!?():;\")\n",
    "            try: stemmed_verb = self.can_verb_be_checked[word]\n",
    "            except: verbs[i] = False; continue\n",
    "        return verbs\n",
    "    \n",
    "    def is_verbs_nutids_r(self, words, verbs_to_check):\n",
    "        is_nutids_r = []\n",
    "        for word, should_check in zip(words, verbs_to_check):\n",
    "            word = word.strip(\",.!?():;\")\n",
    "            if not should_check:\n",
    "                is_nutids_r.append(None)\n",
    "                continue\n",
    "            infinitiv_form, nutids_r_form = self.get_tense_from_verb[self.can_verb_be_checked[word]]\n",
    "            if word == infinitiv_form:\n",
    "                is_nutids_r.append(False)\n",
    "            elif word == nutids_r_form:\n",
    "                is_nutids_r.append(True)\n",
    "            else:\n",
    "                print(\"ERROR: word is not infinitiv or nutids_r\")\n",
    "                is_nutids_r.append(None)\n",
    "        return is_nutids_r\n",
    "    \n",
    "    def make_nutids_r_error_message(self, word_to_correct, all_words_from_sentence, index_of_word_in_all_words, correct_word, to_nutids_r):\n",
    "        previous_index = find_index(all_words_from_sentence, index_of_word_in_all_words, word_to_correct)\n",
    "        nutids_r_comment = \" med nutids-r\"\n",
    "        if to_nutids_r:\n",
    "            # None kan erstattes med \"med nutids-r\", hvis forskellen er et nutids-r\n",
    "            description = f\"{word_to_correct} skal stå i ___ form{nutids_r_comment}, så der står {correct_word}\"\n",
    "        else: \n",
    "            description = f\"{word_to_correct} skal stå i ___ form{nutids_r_comment}, så der står {correct_word}\"\n",
    "        return [word_to_correct, correct_word, previous_index, description]\n",
    "\n",
    "    def make_error_messages(self, words, should_be_nutids_r, is_nutids_r, verbs_to_check):\n",
    "        errors = []\n",
    "        for i in range(len(words)):\n",
    "            if len(words[i].strip(\",.!?():;\")) == 0:\n",
    "                continue\n",
    "            verb_to_check = verbs_to_check[i]\n",
    "            if not verb_to_check:\n",
    "                continue \n",
    "            current_word = words[i].strip(\",.!?():;\")\n",
    "            should_be = should_be_nutids_r[i]\n",
    "            is_nutid = is_nutids_r[i]\n",
    "            if should_be == is_nutid or is_nutid is None:\n",
    "                continue\n",
    "            stemmed_word = self.can_verb_be_checked[current_word]\n",
    "            if should_be is True:\n",
    "                to_nutids_r = True\n",
    "                correct_word = self.get_tense_from_verb[stemmed_word][1]\n",
    "            else:\n",
    "                correct_word = self.get_tense_from_verb[stemmed_word][0]\n",
    "                to_nutids_r = False\n",
    "            correct_word = words[i].replace(current_word, correct_word)\n",
    "            error = self.make_nutids_r_error_message(words[i], words, i, correct_word, to_nutids_r)\n",
    "            errors.append(error)\n",
    "        return errors\n",
    "\n",
    "    def correct(self, sentence, correct_sentence):\n",
    "        words = prepare_sentence(sentence, lowercase=True)\n",
    "        verbs_to_check = self.verbs_to_check(words)\n",
    "        is_nutids_r = self.is_verbs_nutids_r(words, verbs_to_check)\n",
    "        should_be = self.should_be_nutidsr(verbs_to_check)\n",
    "        errors = self.make_error_messages(words, should_be, is_nutids_r, verbs_to_check)\n",
    "        wrong, correct, no_guess = self.get_measures(errors, verbs_to_check, should_be, sentence, correct_sentence)\n",
    "        return errors, (wrong, correct, no_guess)\n",
    "\n",
    "    def get_measures(self, errors, verbs_to_check, should_be_nutids, current_sentence, correct_sentence):\n",
    "        for error in errors:\n",
    "            current_sentence = current_sentence[:error[2][0]] + error[1] + current_sentence[error[2][1]:]\n",
    "            diff = len(error[1]) - len(error[0])\n",
    "            for error in errors:\n",
    "                error[2] = (error[2][0] + diff, error[2][1] + diff)\n",
    "\n",
    "        prediction_words = current_sentence.split()\n",
    "        actual_words = correct_sentence.split()\n",
    "        wrong, correct, no_guess = 0, 0, 0\n",
    "            \n",
    "        for i in range(len(actual_words)):\n",
    "            actual_word = actual_words[i]\n",
    "            prediction_word = prediction_words[i]\n",
    "            should_print = False\n",
    "            if actual_word != prediction_word:\n",
    "                if verbs_to_check[i] and should_be_nutids[i] is not None:\n",
    "                    wrong += 1\n",
    "                    should_print = True\n",
    "                else:\n",
    "                    no_guess += 1\n",
    "            else: \n",
    "                if verbs_to_check[i]:\n",
    "                    correct += 1\n",
    "\n",
    "            if should_print:\n",
    "                print(correct_sentence)\n",
    "                print(current_sentence)\n",
    "        \n",
    "        return wrong, correct, no_guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "\n",
    "class Tester():\n",
    "    def __init__(self, models) -> None:\n",
    "        self.models =  models\n",
    "        self.x = test_sentences_verbs[\"wrong\"]\n",
    "        self.y = test_sentences_verbs[\"correct\"]\n",
    "        self.pos = self.get_pos()\n",
    "\n",
    "    def get_pos_tags(self, sentence):\n",
    "        doc = self.pos_tagger(sentence)\n",
    "        features = [word.feats if word.feats else None for sentence in doc.sentences for word in sentence.words]\n",
    "        feature_dicts = self.turn_features_to_dicts(features)\n",
    "        results = [(word.upos, [word.start_char, word.end_char], feature_dicts[i]) for sentence in doc.sentences for i, word in enumerate(sentence.words)]\n",
    "        return results\n",
    "\n",
    "    def turn_features_to_dicts(self, features):\n",
    "        feature_dicts = []\n",
    "        current_tense = None\n",
    "        for feature in features:\n",
    "            if feature is None:\n",
    "                feature_dicts.append({})\n",
    "                continue\n",
    "            feature_dict = {}\n",
    "            current_features = feature.split(\"|\")\n",
    "            for current_feature in current_features:\n",
    "                key, value = current_feature.split(\"=\")\n",
    "                if key == \"Tense\" and current_tense is None:\n",
    "                    current_tense = value\n",
    "                feature_dict[key] = value\n",
    "            if \"Tense\" not in feature_dict and \"VerbForm\" in feature_dict and key is not None:\n",
    "                feature_dict[\"Tense\"] = \"Pres\" if current_tense is None else current_tense\n",
    "            feature_dicts.append(feature_dict)\n",
    "        return feature_dicts\n",
    "\n",
    "    def get_pos(self):\n",
    "        with open(\"FineTuneModels/pos_caching.pkl\", \"rb\") as f:\n",
    "            pos_list = pickle.load(f)\n",
    "        if len(pos_list) != len(self.x):\n",
    "            pos_list = []\n",
    "            self.pos_tagger = stanza.Pipeline(\"da\", processors='tokenize,pos', use_gpu=True, cache_directory='./cache', tokenize_pretokenized=True, n_process=4)\n",
    "            for sentence in (self.x):\n",
    "                pos = self.get_pos_tags(sentence)\n",
    "                pos_list.append(pos)\n",
    "            print(len(pos_list))\n",
    "            print(\"Updating\")\n",
    "            with open(\"FineTuneModels/pos_caching.pkl\", \"wb\") as f:\n",
    "                pickle.dump(pos_list, f)\n",
    "            print(\"Updated\")\n",
    "        else:\n",
    "            print(\"pos_caching.pkl already exists\")\n",
    "        return pos_list\n",
    "    \n",
    "    def test_one_model(self, model):\n",
    "        total_wrong, total_correct, total_no_guess = 0,0,0\n",
    "        for i in range(len(self.x)):\n",
    "            x, y, pos = self.x[i], self.y[i], self.pos[i]\n",
    "            corrector = NutidsRCorrector(model, pos, 5)\n",
    "            errors, (wrong, correct, no_guess) = corrector.correct(x, y)\n",
    "            total_wrong += wrong\n",
    "            total_correct += correct\n",
    "            total_no_guess += no_guess\n",
    "\n",
    "        return round(total_wrong/(total_wrong+total_correct+total_no_guess)*100, 4), round(total_correct/(total_wrong+total_correct+total_no_guess)*100, 4), round(total_no_guess/(total_wrong+total_correct+total_no_guess)*100, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_caching.pkl already exists\n",
      "Model 1: \n",
      "Wrong:  3.7234\n",
      "Correct:  67.0213\n",
      "No Guess:  29.2553\n",
      "Model 2: \n",
      "Wrong:  3.7234\n",
      "Correct:  67.0213\n",
      "No Guess:  29.2553\n"
     ]
    }
   ],
   "source": [
    "tester = Tester([\"FineTuneModels/models/nutidsrModel2\"])\n",
    "\n",
    "model1 = \"FineTuneModels/models/nutidsrModel2\"\n",
    "model2 = \"FineTuneModels/models/nutidsrModel1\"\n",
    "\n",
    "from IPython.utils import io\n",
    "\n",
    "print(\"Model 1: \")\n",
    "\n",
    "with io.capture_output() as captured:\n",
    "    wrong1, correct1, no_guess1 = tester.test_one_model(load_model(model1))\n",
    "\n",
    "print(\"Wrong: \", wrong1)\n",
    "print(\"Correct: \", correct1)\n",
    "print(\"No Guess: \", no_guess1)\n",
    "\n",
    "print(\"Model 2: \")    \n",
    "with io.capture_output() as captured:   \n",
    "    wrong2, correct2, no_guess2 = tester.test_one_model(load_model(model2))\n",
    "\n",
    "print(\"Wrong: \", wrong2)\n",
    "print(\"Correct: \", correct2)\n",
    "print(\"No Guess: \", no_guess2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
