{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from transformers import Trainer, BertTokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/lucasvilsen/Desktop/GrammatikTAK/\")\n",
    "test_sentences_verbs = pd.read_csv(\"Datasets/EuroparlNutidsr_testset.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load_model() missing 1 required positional argument: 'path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m     classifier\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Trainer(classifier)\n\u001b[0;32m---> 15\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mNutidsRCorrector\u001b[39;00m():\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pos, padding):\n",
      "\u001b[0;31mTypeError\u001b[0m: load_model() missing 1 required positional argument: 'path'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from GrammatiktakBackend.Utilities.utils import find_index, prepare_sentence\n",
    "from transformers import pipeline\n",
    "import tqdm\n",
    "tqdm.disable = True\n",
    "\n",
    "def load_model(path):\n",
    "    device = \"mps\"\n",
    "    torch.device(device)\n",
    "    classifier = torch.load(path, map_location=torch.device('cpu'))\n",
    "    classifier.eval()\n",
    "    classifier.to(device)\n",
    "    return Trainer(classifier)\n",
    "\n",
    "model = load_model()\n",
    "\n",
    "class NutidsRCorrector():\n",
    "    def __init__(self, pos, padding):\n",
    "        self.can_verb_be_checked = pickle.load(open(\"Datasets/nutids_r_stem.pickle\", \"rb\"))\n",
    "        self.get_tense_from_verb = pickle.load(open(\"Datasets/nutids_r_bøjninger.pickle\", \"rb\"))\n",
    "        self.classifier = model\n",
    "        self.padding = padding\n",
    "        self.pos = pos\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('Maltehb/danish-bert-botxo')\n",
    "\n",
    "    def should_be_nutidsr(self, verbs_to_check):\n",
    "        dataset = self.make_dataset(verbs_to_check)\n",
    "        if len(dataset) < 1:\n",
    "            return [None]*len(verbs_to_check)\n",
    "        tokenized = self.tokenize_sentences(dataset)\n",
    "        dataloader = self.convert_dataset_to_dataloader(tokenized)\n",
    "        predictions = self.get_predictions(dataloader)\n",
    "        return list(self.turn_predictions_to_bool(predictions, verbs_to_check))\n",
    "\n",
    "    def turn_predictions_to_bool(self, predictions, verbs_to_check):\n",
    "        prediction_index = 0\n",
    "        for i in range(len(verbs_to_check)):\n",
    "            if verbs_to_check[i]:\n",
    "                if predictions[prediction_index] == 0:\n",
    "                    yield True\n",
    "                yield False\n",
    "                prediction_index += 1\n",
    "            else:\n",
    "                yield None\n",
    "\n",
    "    def convert_dataset_to_dataloader(self, dataset):\n",
    "        test_dataset = Dataset(dataset)\n",
    "        return test_dataset\n",
    "    \n",
    "    def tokenize_sentences(self, sentences):\n",
    "        X_tokenized = self.tokenizer(sentences, padding=True, truncation=True, max_length=26)\n",
    "        return X_tokenized\n",
    "    \n",
    "    def get_predictions(self, dataloader):\n",
    "        raw_predictions, _, _ = self.classifier.predict(dataloader)\n",
    "        final_prediction = np.argmax(raw_predictions, axis=1)\n",
    "        return final_prediction\n",
    "\n",
    "    def make_dataset(self, verbs_to_check):\n",
    "        pos_with_padding = [\"<PAD>\"]*self.padding + [p[0] for p in self.pos] + [\"<PAD>\"]*self.padding\n",
    "        dataset = []\n",
    "        for i in range(len(verbs_to_check)):\n",
    "            if verbs_to_check[i]:\n",
    "                dataset.append(\" \".join(pos_with_padding[i:i+2*self.padding+1]))\n",
    "        return dataset\n",
    "\n",
    "    def verbs_to_check(self, words):\n",
    "        pos = self.pos\n",
    "        verbs = []\n",
    "        for i in range(len(pos)):\n",
    "            if pos[i][0] != \"VERB\":\n",
    "                verbs.append(False)\n",
    "            elif \"Tense\" not in pos[i][2].keys():\n",
    "                verbs.append(False)\n",
    "            elif pos[i][2][\"Tense\"] != \"Pres\":\n",
    "                verbs.append(False)\n",
    "            else:\n",
    "                verbs.append(True)\n",
    "        for i, bool in enumerate(verbs):\n",
    "            if not bool:\n",
    "                continue\n",
    "            word = words[i].strip(\",.!?():;\")\n",
    "            try: stemmed_verb = self.can_verb_be_checked[word]\n",
    "            except: verbs[i] = False; continue\n",
    "        return verbs\n",
    "    \n",
    "    def is_verbs_nutids_r(self, words, verbs_to_check):\n",
    "        is_nutids_r = []\n",
    "        for word, should_check in zip(words, verbs_to_check):\n",
    "            word = word.strip(\",.!?():;\")\n",
    "            if not should_check:\n",
    "                is_nutids_r.append(None)\n",
    "                continue\n",
    "            infinitiv_form, nutids_r_form = self.get_tense_from_verb[self.can_verb_be_checked[word]]\n",
    "            if word == infinitiv_form:\n",
    "                is_nutids_r.append(False)\n",
    "            elif word == nutids_r_form:\n",
    "                is_nutids_r.append(True)\n",
    "            else:\n",
    "                print(\"ERROR: word is not infinitiv or nutids_r\")\n",
    "                is_nutids_r.append(None)\n",
    "        return is_nutids_r\n",
    "    \n",
    "    def make_nutids_r_error_message(self, word_to_correct, all_words_from_sentence, index_of_word_in_all_words, correct_word, to_nutids_r):\n",
    "        previous_index = find_index(all_words_from_sentence, index_of_word_in_all_words, word_to_correct)\n",
    "        nutids_r_comment = \" med nutids-r\"\n",
    "        if to_nutids_r:\n",
    "            # None kan erstattes med \"med nutids-r\", hvis forskellen er et nutids-r\n",
    "            description = f\"{word_to_correct} skal stå i ___ form{nutids_r_comment}, så der står {correct_word}\"\n",
    "        else: \n",
    "            description = f\"{word_to_correct} skal stå i ___ form{nutids_r_comment}, så der står {correct_word}\"\n",
    "        return [word_to_correct, correct_word, previous_index, description]\n",
    "\n",
    "    def make_error_messages(self, words, should_be_nutids_r, is_nutids_r, verbs_to_check):\n",
    "        errors = []\n",
    "        for i in range(len(words)):\n",
    "            if len(words[i].strip(\",.!?():;\")) == 0:\n",
    "                continue\n",
    "            verb_to_check = verbs_to_check[i]\n",
    "            if not verb_to_check:\n",
    "                continue \n",
    "            current_word = words[i].strip(\",.!?():;\")\n",
    "            should_be = should_be_nutids_r[i]\n",
    "            is_nutid = is_nutids_r[i]\n",
    "            if should_be == is_nutid or is_nutid is None:\n",
    "                continue\n",
    "            stemmed_word = self.can_verb_be_checked[current_word]\n",
    "            if should_be is True:\n",
    "                to_nutids_r = True\n",
    "                correct_word = self.get_tense_from_verb[stemmed_word][1]\n",
    "            else:\n",
    "                correct_word = self.get_tense_from_verb[stemmed_word][0]\n",
    "                to_nutids_r = False\n",
    "            correct_word = words[i].replace(current_word, correct_word)\n",
    "            error = self.make_nutids_r_error_message(words[i], words, i, correct_word, to_nutids_r)\n",
    "            errors.append(error)\n",
    "        return errors\n",
    "\n",
    "    def correct(self, sentence, correct_sentence):\n",
    "        words = prepare_sentence(sentence, lowercase=True)\n",
    "        verbs_to_check = self.verbs_to_check(words)\n",
    "        is_nutids_r = self.is_verbs_nutids_r(words, verbs_to_check)\n",
    "        should_be = self.should_be_nutidsr(verbs_to_check)\n",
    "        errors = self.make_error_messages(words, should_be, is_nutids_r, verbs_to_check)\n",
    "        wrong, correct, no_guess = self.get_measures(errors, verbs_to_check, should_be, sentence, correct_sentence)\n",
    "        return errors, (wrong, correct, no_guess)\n",
    "\n",
    "    def get_measures(self, errors, verbs_to_check, should_be_nutids, current_sentence, correct_sentence):\n",
    "        for error in errors:\n",
    "            current_sentence = current_sentence[:error[2][0]] + error[1] + current_sentence[error[2][1]:]\n",
    "            diff = len(error[1]) - len(error[0])\n",
    "            for error in errors:\n",
    "                error[2] = (error[2][0] + diff, error[2][1] + diff)\n",
    "\n",
    "        prediction_words = current_sentence.split()\n",
    "        actual_words = correct_sentence.split()\n",
    "        wrong, correct, no_guess = 0, 0, 0\n",
    "            \n",
    "        for i in range(len(actual_words)):\n",
    "            actual_word = actual_words[i]\n",
    "            prediction_word = prediction_words[i]\n",
    "            should_print = False\n",
    "            if actual_word != prediction_word:\n",
    "                if verbs_to_check[i] and should_be_nutids[i] is not None:\n",
    "                    wrong += 1\n",
    "                    should_print = True\n",
    "                else:\n",
    "                    no_guess += 1\n",
    "            else: \n",
    "                if verbs_to_check[i]:\n",
    "                    correct += 1\n",
    "\n",
    "            if should_print:\n",
    "                print(correct_sentence)\n",
    "                print(current_sentence)\n",
    "        \n",
    "        return wrong, correct, no_guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "\n",
    "class Tester():\n",
    "    def __init__(self, models) -> None:\n",
    "        self.models =  models\n",
    "        self.x = test_sentences_verbs[\"wrong\"]\n",
    "        self.y = test_sentences_verbs[\"correct\"]\n",
    "        self.pos = self.get_pos()\n",
    "\n",
    "    def get_pos_tags(self, sentence):\n",
    "        doc = self.pos_tagger(sentence)\n",
    "        features = [word.feats if word.feats else None for sentence in doc.sentences for word in sentence.words]\n",
    "        feature_dicts = self.turn_features_to_dicts(features)\n",
    "        results = [(word.upos, [word.start_char, word.end_char], feature_dicts[i]) for sentence in doc.sentences for i, word in enumerate(sentence.words)]\n",
    "        return results\n",
    "\n",
    "    def turn_features_to_dicts(self, features):\n",
    "        feature_dicts = []\n",
    "        current_tense = None\n",
    "        for feature in features:\n",
    "            if feature is None:\n",
    "                feature_dicts.append({})\n",
    "                continue\n",
    "            feature_dict = {}\n",
    "            current_features = feature.split(\"|\")\n",
    "            for current_feature in current_features:\n",
    "                key, value = current_feature.split(\"=\")\n",
    "                if key == \"Tense\" and current_tense is None:\n",
    "                    current_tense = value\n",
    "                feature_dict[key] = value\n",
    "            if \"Tense\" not in feature_dict and \"VerbForm\" in feature_dict and key is not None:\n",
    "                feature_dict[\"Tense\"] = \"Pres\" if current_tense is None else current_tense\n",
    "            feature_dicts.append(feature_dict)\n",
    "        return feature_dicts\n",
    "\n",
    "    def get_pos(self):\n",
    "        with open(\"FineTuneModels/pos_caching.pkl\", \"rb\") as f:\n",
    "            pos_list = pickle.load(f)\n",
    "        if len(pos_list) != len(self.x):\n",
    "            pos_list = []\n",
    "            self.pos_tagger = stanza.Pipeline(\"da\", processors='tokenize,pos', use_gpu=True, cache_directory='./cache', tokenize_pretokenized=True, n_process=4)\n",
    "            for sentence in (self.x):\n",
    "                pos = self.get_pos_tags(sentence)\n",
    "                pos_list.append(pos)\n",
    "            print(len(pos_list))\n",
    "            print(\"Updating\")\n",
    "            with open(\"FineTuneModels/pos_caching.pkl\", \"wb\") as f:\n",
    "                pickle.dump(pos_list, f)\n",
    "            print(\"Updated\")\n",
    "        else:\n",
    "            print(\"pos_caching.pkl already exists\")\n",
    "        return pos_list\n",
    "    \n",
    "    def test_one_model(self, model):\n",
    "        total_wrong, total_correct, total_no_guess = 0,0,0\n",
    "        for i in range(len(self.x)):\n",
    "            x, y, pos = self.x[i], self.y[i], self.pos[i]\n",
    "            corrector = NutidsRCorrector(pos, 5)\n",
    "            errors, (wrong, correct, no_guess) = corrector.correct(x, y)\n",
    "            total_wrong += wrong\n",
    "            total_correct += correct\n",
    "            total_no_guess += no_guess\n",
    "        \n",
    "        print(\"Total wrong: \", round(total_wrong/(total_wrong+total_correct+total_no_guess)*100, 4))\n",
    "        print(\"Total correct: \", round(total_correct/(total_wrong+total_correct+total_no_guess)*100, 4))\n",
    "        print(\"Total no guess: \", round(total_no_guess/(total_wrong+total_correct+total_no_guess)*100, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_caching.pkl already exists\n",
      "Model 1: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34da7af6ab2b4d83a52052135eb37c77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8798da6819e643a2b6a9cf009d2d82ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "169c5769a9594977b06b5d9a5bf1243c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73ce15012f844882ad69b65cbfd406bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6d6e17f51f940b792c8f32e075644f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6da90ea4f620492cacca82a1aecc819d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f1d1f1cda9f444896ad9e404ab7c979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06d42d1eb3d94ba098001b7d449415ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Betænkningen af Cunha om flerårige udviklingsprogrammer skal forhandles af Parlamentet på torsdag og indeholder et forslag i punkt 6 om, at der skal indføres kvotesanktioner for lande, der ikke overholder deres årlige målsætninger for flådereduktion.\n",
      "Betænkningen af Cunha om flerårige udviklingsprogrammer skal forhandles af Parlamentet på torsdag og indeholde et forslag i punkt 6 om, at der skal indføres kvotesanktioner for lande, der ikke overholder deres årlige målsætninger for flådereduktion.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75378e68ca684d3c8e54dbc1a6fc7793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2f6e2276ef74fdea0e31f2343abcae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be49bb5d8b9247eb93ada34590b82f96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5421b4725bcd447484ce0274c89d0d46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "debc70b6921e441e910f75708bdb9d5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38925ed7b9ca4ab098077382ee3ed17a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd9a8e062b5346faaf53c42d379e8ae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25b03c23572c439984488ff83335250d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c25cfc204754b8caf48ee5408498a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d1f37c6a41a4b13bb8e691afe2d1a7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11392d6f6eea4e79a9c7e40cd00796ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ja, fru Schroedter, jeg skal med glæde undersøge dette spørgsmål, når jeg har modtaget Deres brev.\n",
      "Ja, fru Schroedter, jeg skal med glæde undersøger dette spørgsmål, når jeg har modtaget Deres brev.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12e1d26975ec43f6a1de5dfb750124fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0a27be2e2604c08948b53d60dd90cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d8fb07cae9842089b9c3733237f8f23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ce4eb82a57943b9ab3b44151b67e2a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05766af51dad4268b82d9d7ac0b786f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Det vil, håber jeg, blive behandlet i en positiv ånd.\n",
      "Det vil, håbe jeg, bliver behandlet i en positiv ånd.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39cbee70db0c42669264cc1a1387f39e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fru formand, kan De fortælle mig, hvorfor Parlamentet ikke overholder de lovgivningsbestemmelser om sundhed og sikkerhed, som det selv har fastsat?\n",
      "Fru formand, kan De fortælle mig, hvorfor Parlamentet ikke overholde de lovgivningsbestemmelser om sundhed og sikkerhed, som det selv har fastsat?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "097f59e6eaa949e39c77b4d4c163e7e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a14d9b94ba741b09a486f0f8da0ecdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cac8fe6f0d94ff2a8cd39a5486d5b2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c32c5fd959624352a1cf95dfbd4eae1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c60d04d44364b908d92ef1dde872330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8141e8ee027e42c0b7d367823feb2d8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dd21af649ac4181a9991ce422f6fba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "716c25f34f594c62b9043505dec3a17c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30d1b7cadee14c80ad1423dffbc79670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6cb8bde53d44c5bb7d1bc140f1ea8f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tester = Tester([\"FineTuneModels/models/nutidsrModel2\"])\n",
    "\n",
    "model1 = \"FineTuneModels/models/nutidsrModel2\"\n",
    "model2 = \"FineTuneModels/models/nutidsrModel1\"\n",
    "\n",
    "print(\"Model 1: \")\n",
    "tester.test_one_model(load_model(model1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model 2: \")\n",
    "tester.test_one_model(load_model(model2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
