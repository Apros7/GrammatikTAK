{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PUNCT PRON AUX VERB NOUN DET ADJ NOUN ADP PUNC...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NOUN VERB PRON PUNCT ADP ADV DET NOUN NOUN AUX...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2461873</th>\n",
       "      <td>&lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; PRON VERB ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2461874</th>\n",
       "      <td>&lt;PAD&gt; PRON VERB SCONJ PRON AUX VERB ADV ADP PA...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2461875</th>\n",
       "      <td>AUX VERB ADV ADP PART VERB NOUN CCONJ ADP NOUN...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2461876</th>\n",
       "      <td>&lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2461877</th>\n",
       "      <td>&lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2461878 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              comment_text  label\n",
       "0        <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD...      0\n",
       "1        PUNCT PRON AUX VERB NOUN DET ADJ NOUN ADP PUNC...      1\n",
       "2        <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD...      0\n",
       "3        NOUN VERB PRON PUNCT ADP ADV DET NOUN NOUN AUX...      0\n",
       "4        <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD...      0\n",
       "...                                                    ...    ...\n",
       "2461873  <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> PRON VERB ...      1\n",
       "2461874  <PAD> PRON VERB SCONJ PRON AUX VERB ADV ADP PA...      1\n",
       "2461875  AUX VERB ADV ADP PART VERB NOUN CCONJ ADP NOUN...      0\n",
       "2461876  <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD...      0\n",
       "2461877  <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD...      0\n",
       "\n",
       "[2461878 rows x 2 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../Datasets/EuroparlNutidsr_trainset_verbs.csv\", sep=\";\")\n",
    "original_pos = list(df[\"comment_text\"].values)\n",
    "original_labels = list(df[\"label\"].values)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"/Users/lucasvilsen/Desktop/GrammatikTAK/Datasets/\")\n",
    "filename = \"europarl-v7.da-en.da\"\n",
    "with open(filename, \"r\", encoding=\"UTF-8\") as file:\n",
    "    lines = file.readlines()\n",
    "correct_sentences = lines[-5000:]\n",
    "correct_sentences = [line.strip(\"\\n\") for line in correct_sentences]\n",
    "len(correct_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spelling_errors.csv', 'README.md', 'danish_ner.pickle', 'present_tense.csv']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "244"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from transformers import Trainer, BertTokenizer\n",
    "import numpy as np\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "\n",
    "os.chdir(\"/Users/lucasvilsen/Desktop/GrammatiktakDatasets/Danish\")\n",
    "print(os.listdir())\n",
    "\n",
    "test_sentences_verbs = pd.read_csv(\"present_tense.csv\", sep=\"|\")\n",
    "correct_sentences = test_sentences_verbs[\"correct\"].values\n",
    "len(correct_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Liikanen, om De hurtigt vil afklare dette med Deres kollega, hr.'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_caching.pkl already exists\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"/Users/lucasvilsen/Desktop/GrammatikTAK/\")\n",
    "\n",
    "import pickle\n",
    "import stanza\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "def get_pos(x):\n",
    "    if os.path.exists(f\"FineTuneModels/cache/pos_caching_{len(x)}.pkl\"):\n",
    "        print(\"pos_caching.pkl already exists\")\n",
    "        with open(f\"FineTuneModels/cache/pos_caching_{len(x)}.pkl\", \"rb\") as f:\n",
    "            pos_list = pickle.load(f)\n",
    "    else: \n",
    "        pos_list = []\n",
    "        pos_tagger = stanza.Pipeline(\"da\", processors='tokenize,pos', use_gpu=True, cache_directory='./cache', tokenize_pretokenized=True, n_process=4)\n",
    "        for sentence in tqdm(x):\n",
    "            pos = get_pos_tags(sentence, pos_tagger)\n",
    "            pos_list.append(pos)\n",
    "        print(len(pos_list))\n",
    "        print(\"Updating\")\n",
    "        with open(f\"FineTuneModels/cache/pos_caching_{len(x)}.pkl\", \"wb\") as f:\n",
    "            pickle.dump(pos_list, f)\n",
    "        print(\"Updated\")\n",
    "    return pos_list\n",
    "\n",
    "def get_pos_tags(sentence, pos_tagger):\n",
    "    doc = pos_tagger(sentence)\n",
    "    features = [word.feats if word.feats else None for sentence in doc.sentences for word in sentence.words]\n",
    "    feature_dicts = turn_features_to_dicts(features)\n",
    "    results = [(word.upos, [word.start_char, word.end_char], feature_dicts[i]) for sentence in doc.sentences for i, word in enumerate(sentence.words)]\n",
    "    return results\n",
    "\n",
    "def turn_features_to_dicts(features):\n",
    "    feature_dicts = []\n",
    "    current_tense = None\n",
    "    for feature in features:\n",
    "        if feature is None:\n",
    "            feature_dicts.append({})\n",
    "            continue\n",
    "        feature_dict = {}\n",
    "        current_features = feature.split(\"|\")\n",
    "        for current_feature in current_features:\n",
    "            key, value = current_feature.split(\"=\")\n",
    "            if key == \"Tense\" and current_tense is None:\n",
    "                current_tense = value\n",
    "            feature_dict[key] = value\n",
    "        if \"Tense\" not in feature_dict and \"VerbForm\" in feature_dict and key is not None:\n",
    "            feature_dict[\"Tense\"] = \"Pres\" if current_tense is None else current_tense\n",
    "        feature_dicts.append(feature_dict)\n",
    "    return feature_dicts\n",
    "\n",
    "all_pos = get_pos(correct_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 46066.35it/s]\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"/Users/lucasvilsen/Desktop/GrammatikTAK/Datasets/\")\n",
    "filename = \"europarl-v7.da-en.da\"\n",
    "with open(filename, \"r\", encoding=\"UTF-8\") as file:\n",
    "    lines = file.readlines()\n",
    "with open(\"nutids_r_bøjninger.pickle\", \"rb\") as f:\n",
    "    nutids_r_bøjninger = pickle.load(f)\n",
    "with open(\"nutids_r_stem.pickle\", \"rb\") as f:\n",
    "    nutids_r_stem = pickle.load(f)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "testset = []\n",
    "labels = []\n",
    "\n",
    "padded_words = []\n",
    "\n",
    "padding_left = 3\n",
    "padding_right = 1\n",
    "\n",
    "def get_pos_tags(index):\n",
    "    current_pos = all_pos[index]\n",
    "    return [current_pos[i][0] for i in range(len(current_pos))]\n",
    "\n",
    "og_index = 0\n",
    "comma_right_before_index = 0\n",
    "at_indexes = []\n",
    "at_index = -1\n",
    "\n",
    "\n",
    "for y in tqdm(range(len(correct_sentences))):\n",
    "    line = correct_sentences[y]\n",
    "    if len(str(line)) < 1 or str(line) == \"nan\":\n",
    "        continue\n",
    "    line = line.strip(\"\\n\")\n",
    "    true_words = line.split()\n",
    "    pos = get_pos_tags(y)\n",
    "    words = [\"<PAD>\"]*padding_left + pos + [\"<PAD>\"]*padding_right\n",
    "    true_padded_words = [\"<PAD>\"]*padding_left + true_words + [\"<PAD>\"]*padding_right\n",
    "    for i, word in enumerate(true_words):\n",
    "        try: stemmed = nutids_r_stem[word]\n",
    "        except: continue\n",
    "        if word[-1] == \"s\" or words[i+padding_left] != \"VERB\":\n",
    "            continue\n",
    "        if true_words[i-1].lower().strip() == \"og\": \n",
    "            og_index += 1\n",
    "            continue\n",
    "        if true_words[i-1][-1] == \",\":\n",
    "            comma_right_before_index += 1\n",
    "            continue\n",
    "        at_index += 1\n",
    "        if true_words[i-1].lower().strip() == \"at\": \n",
    "            at_indexes.append(at_index)\n",
    "        if nutids_r_bøjninger[stemmed][0] == word:\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "        testset.append(\" \".join(words[i:i+padding_left+padding_right+1]))\n",
    "        padded_words.append(\" \".join(true_padded_words[i:i+padding_left+padding_right+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_testset = [\" \".join(sent_pos.split()) for sent_pos in testset]\n",
    "new_pos_and_labels = [(\" \".join(sent_pos.split()[15-padding_left:16+padding_right]), label) for sent_pos, label in zip(original_pos, original_labels)]\n",
    "new_pos = [x[0] for x in new_pos_and_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2461878/2461878 [00:00<00:00, 2715312.06it/s]\n"
     ]
    }
   ],
   "source": [
    "duplicates_dict = {}\n",
    "duplicates = []\n",
    "for element in tqdm(new_pos_and_labels):\n",
    "    if element[0] in duplicates_dict:\n",
    "        if element[1] != duplicates_dict[element[0]]:\n",
    "            duplicates.append(element[0])\n",
    "    else:\n",
    "        duplicates_dict[element[0]] = element[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6011, 17755, 17755)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(duplicates)), len(duplicates_dict), len(set(new_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2461878 2461878\n",
      "PRON ADV AUX VERB PRON\n",
      "<PAD> <PAD> PRON VERB NOUN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5921/5921 [00:16<00:00, 349.38it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1820"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(new_pos), len(original_pos))\n",
    "print(new_testset[0])\n",
    "print(new_pos[0])\n",
    "\n",
    "from tqdm import tqdm\n",
    "can_guess = 0\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for sent_pos in tqdm(new_testset):\n",
    "    if sent_pos in duplicates_dict and sent_pos not in duplicates:\n",
    "        can_guess += 1\n",
    "        predictions.append(duplicates_dict[sent_pos])\n",
    "    else:\n",
    "        predictions.append(None)\n",
    "\n",
    "can_guess "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5921, 5921)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct:  30.64 %\n",
      "Wrong:  0.1 %\n",
      "No guess:  69.26 %\n",
      "Correct to wrong ratio:  302.33\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array(predictions)\n",
    "b = np.array(labels)\n",
    "\n",
    "no_guess = 0\n",
    "correct = 0\n",
    "wrong = 0\n",
    "\n",
    "for p, l in zip(predictions, labels):\n",
    "    if p is None:\n",
    "        no_guess += 1\n",
    "        continue\n",
    "    if p == l:\n",
    "        correct += 1\n",
    "    else:\n",
    "        wrong += 1\n",
    "    \n",
    "no_guess = no_guess / len(predictions) * 100\n",
    "correct = correct / len(predictions) * 100\n",
    "wrong = wrong / len(predictions) * 100\n",
    "\n",
    "print(\"Correct: \", round(correct, 2), \"%\")\n",
    "print(\"Wrong: \", round(wrong, 2), \"%\")\n",
    "print(\"No guess: \", round(no_guess, 2), \"%\")\n",
    "print(\"Correct to wrong ratio: \", round(correct/wrong, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct:  [30, 53, 64, 63, 53, 41, 30, 23]\n",
      "Ratios:  [302, 196, 123, 114, 137, 202, 137, 139]\n",
      "Final combined score:  [1.3, 1.18, 1.05, 1.01, 0.98, 1.08, 0.75, 0.69]\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "correct = [30, 53, 64, 63, 53, 41, 30, 23]\n",
    "ratio = [302, 196, 123, 114, 137, 202, 137, 139]\n",
    "procentwise_ratio = [round(el/302, 2) for el in ratio]\n",
    "final_score = [round(x/100 + y, 2) for x,y in zip(correct, procentwise_ratio)]\n",
    "\n",
    "print(\"Correct: \", correct)\n",
    "print(\"Ratios: \", ratio)\n",
    "print(\"Final combined score: \", final_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thereby the best dict method is:\n",
    "\n",
    "1. 3-1\n",
    "2. 4-1\n",
    "3. 8-1\n",
    "4. 5-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
