{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PUNCT PRON AUX VERB NOUN DET ADJ NOUN ADP PUNC...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NOUN VERB PRON PUNCT ADP ADV DET NOUN NOUN AUX...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2461873</th>\n",
       "      <td>&lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; PRON VERB ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2461874</th>\n",
       "      <td>&lt;PAD&gt; PRON VERB SCONJ PRON AUX VERB ADV ADP PA...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2461875</th>\n",
       "      <td>AUX VERB ADV ADP PART VERB NOUN CCONJ ADP NOUN...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2461876</th>\n",
       "      <td>&lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2461877</th>\n",
       "      <td>&lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2461878 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              comment_text  label\n",
       "0        <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD...      0\n",
       "1        PUNCT PRON AUX VERB NOUN DET ADJ NOUN ADP PUNC...      1\n",
       "2        <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD...      0\n",
       "3        NOUN VERB PRON PUNCT ADP ADV DET NOUN NOUN AUX...      0\n",
       "4        <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD...      0\n",
       "...                                                    ...    ...\n",
       "2461873  <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> PRON VERB ...      1\n",
       "2461874  <PAD> PRON VERB SCONJ PRON AUX VERB ADV ADP PA...      1\n",
       "2461875  AUX VERB ADV ADP PART VERB NOUN CCONJ ADP NOUN...      0\n",
       "2461876  <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD...      0\n",
       "2461877  <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD...      0\n",
       "\n",
       "[2461878 rows x 2 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../Datasets/EuroparlNutidsr_trainset_verbs.csv\", sep=\";\")\n",
    "all_pos = list(df[\"comment_text\"].values)\n",
    "all_labels = list(df[\"label\"].values)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/lucasvilsen/Desktop/GrammatikTAK/Datasets/\")\n",
    "filename = \"europarl-v7.da-en.da\"\n",
    "with open(filename, \"r\", encoding=\"UTF-8\") as file:\n",
    "    lines = file.readlines()\n",
    "correct_sentences = lines[-5000:]\n",
    "correct_sentences = [line.strip(\"\\n\") for line in correct_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Liikanen, om De hurtigt vil afklare dette med Deres kollega, hr.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_caching.pkl already exists\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"/Users/lucasvilsen/Desktop/GrammatikTAK/\")\n",
    "\n",
    "import pickle\n",
    "import stanza\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "def get_pos(x):\n",
    "    if os.path.exists(f\"FineTuneModels/cache/pos_caching_{len(x)}.pkl\"):\n",
    "        print(\"pos_caching.pkl already exists\")\n",
    "        with open(f\"FineTuneModels/cache/pos_caching_{len(x)}.pkl\", \"rb\") as f:\n",
    "            pos_list = pickle.load(f)\n",
    "    else: \n",
    "        pos_list = []\n",
    "        pos_tagger = stanza.Pipeline(\"da\", processors='tokenize,pos', use_gpu=True, cache_directory='./cache', tokenize_pretokenized=True, n_process=4)\n",
    "        for sentence in tqdm(x):\n",
    "            pos = get_pos_tags(sentence, pos_tagger)\n",
    "            pos_list.append(pos)\n",
    "        print(len(pos_list))\n",
    "        print(\"Updating\")\n",
    "        with open(f\"FineTuneModels/cache/pos_caching_{len(x)}.pkl\", \"wb\") as f:\n",
    "            pickle.dump(pos_list, f)\n",
    "        print(\"Updated\")\n",
    "    return pos_list\n",
    "\n",
    "def get_pos_tags(sentence, pos_tagger):\n",
    "    doc = pos_tagger(sentence)\n",
    "    features = [word.feats if word.feats else None for sentence in doc.sentences for word in sentence.words]\n",
    "    feature_dicts = turn_features_to_dicts(features)\n",
    "    results = [(word.upos, [word.start_char, word.end_char], feature_dicts[i]) for sentence in doc.sentences for i, word in enumerate(sentence.words)]\n",
    "    return results\n",
    "\n",
    "def turn_features_to_dicts(features):\n",
    "    feature_dicts = []\n",
    "    current_tense = None\n",
    "    for feature in features:\n",
    "        if feature is None:\n",
    "            feature_dicts.append({})\n",
    "            continue\n",
    "        feature_dict = {}\n",
    "        current_features = feature.split(\"|\")\n",
    "        for current_feature in current_features:\n",
    "            key, value = current_feature.split(\"=\")\n",
    "            if key == \"Tense\" and current_tense is None:\n",
    "                current_tense = value\n",
    "            feature_dict[key] = value\n",
    "        if \"Tense\" not in feature_dict and \"VerbForm\" in feature_dict and key is not None:\n",
    "            feature_dict[\"Tense\"] = \"Pres\" if current_tense is None else current_tense\n",
    "        feature_dicts.append(feature_dict)\n",
    "    return feature_dicts\n",
    "\n",
    "all_pos = get_pos(correct_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 44866.35it/s]\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"/Users/lucasvilsen/Desktop/GrammatikTAK/Datasets/\")\n",
    "filename = \"europarl-v7.da-en.da\"\n",
    "with open(filename, \"r\", encoding=\"UTF-8\") as file:\n",
    "    lines = file.readlines()\n",
    "with open(\"nutids_r_bøjninger.pickle\", \"rb\") as f:\n",
    "    nutids_r_bøjninger = pickle.load(f)\n",
    "with open(\"nutids_r_stem.pickle\", \"rb\") as f:\n",
    "    nutids_r_stem = pickle.load(f)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "testset = []\n",
    "labels = []\n",
    "\n",
    "padded_words = []\n",
    "\n",
    "padding_left = 15\n",
    "padding_right = 5\n",
    "\n",
    "def get_pos_tags(index):\n",
    "    current_pos = all_pos[index]\n",
    "    return [current_pos[i][0] for i in range(len(current_pos))]\n",
    "\n",
    "og_index = 0\n",
    "comma_right_before_index = 0\n",
    "at_indexes = []\n",
    "at_index = -1\n",
    "\n",
    "\n",
    "for y in tqdm(range(len(correct_sentences))):\n",
    "    line = correct_sentences[y]\n",
    "    if len(str(line)) < 1 or str(line) == \"nan\":\n",
    "        continue\n",
    "    line = line.strip(\"\\n\")\n",
    "    true_words = line.split()\n",
    "    pos = get_pos_tags(y)\n",
    "    words = [\"<PAD>\"]*padding_left + pos + [\"<PAD>\"]*padding_right\n",
    "    true_padded_words = [\"<PAD>\"]*padding_left + true_words + [\"<PAD>\"]*padding_right\n",
    "    for i, word in enumerate(true_words):\n",
    "        try: stemmed = nutids_r_stem[word]\n",
    "        except: continue\n",
    "        if word[-1] == \"s\" or words[i+padding_left] != \"VERB\":\n",
    "            continue\n",
    "        if true_words[i-1].lower().strip() == \"og\": \n",
    "            og_index += 1\n",
    "            continue\n",
    "        if true_words[i-1][-1] == \",\":\n",
    "            comma_right_before_index += 1\n",
    "            continue\n",
    "        at_index += 1\n",
    "        if true_words[i-1].lower().strip() == \"at\": \n",
    "            at_indexes.append(at_index)\n",
    "        if nutids_r_bøjninger[stemmed][0] == word:\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "        testset.append(\" \".join(words[i:i+padding_left+padding_right+1]))\n",
    "        padded_words.append(\" \".join(true_padded_words[i:i+padding_left+padding_right+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5921/5921 [00:00<00:00, 20490.42it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "680"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_all_pos = []\n",
    "\n",
    "for sent_pos in all_pos:\n",
    "    temp = []\n",
    "    for tup in sent_pos:\n",
    "        temp.append(tup[0])\n",
    "\n",
    "    simple_all_pos.append(temp)\n",
    "\n",
    "new_testset = [\" \".join(sent_pos.split()[11:17]) for sent_pos in testset]\n",
    "new_pos = [\" \".join(sent_pos[11:17]) for sent_pos in simple_all_pos]\n",
    "\n",
    "from tqdm import tqdm\n",
    "can_guess = 0\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for sent_pos in tqdm(new_testset):\n",
    "    if sent_pos in new_pos:\n",
    "        can_guess += 1\n",
    "        predictions.append(all_labels[new_pos.index(sent_pos)])\n",
    "    else:\n",
    "        predictions.append(None)\n",
    "\n",
    "can_guess "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5921, 5921)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88.51545347069751, 4.847154196926195, 6.637392332376288)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array(predictions)\n",
    "b = np.array(labels)\n",
    "\n",
    "no_guess = 0\n",
    "correct = 0\n",
    "wrong = 0\n",
    "\n",
    "for p, l in zip(predictions, labels):\n",
    "    if p is None:\n",
    "        no_guess += 1\n",
    "        continue\n",
    "    if p == l:\n",
    "        correct += 1\n",
    "    else:\n",
    "        wrong += 1\n",
    "    \n",
    "no_guess = no_guess / len(predictions) * 100\n",
    "correct = correct / len(predictions) * 100\n",
    "wrong = wrong / len(predictions) * 100\n",
    "\n",
    "no_guess, correct, wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
