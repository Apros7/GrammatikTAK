{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning Bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /opt/homebrew/lib/python3.10/site-packages (0.11.0)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.10/site-packages (from huggingface_hub) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/lib/python3.10/site-packages (from huggingface_hub) (4.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.10/site-packages (from huggingface_hub) (6.0)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.10/site-packages (from huggingface_hub) (2.28.1)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.10/site-packages (from huggingface_hub) (3.8.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/lucasvilsen/Library/Python/3.10/lib/python/site-packages (from huggingface_hub) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/lucasvilsen/Library/Python/3.10/lib/python/site-packages (from packaging>=20.9->huggingface_hub) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/homebrew/lib/python3.10/site-packages (from requests->huggingface_hub) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.10/site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.10/site-packages (from requests->huggingface_hub) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.12)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in /opt/homebrew/lib/python3.10/site-packages (4.24.0)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.10/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/lib/python3.10/site-packages (from transformers) (2022.9.13)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.10/site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/homebrew/lib/python3.10/site-packages (from transformers) (0.13.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/homebrew/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/lucasvilsen/Library/Python/3.10/lib/python/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/homebrew/lib/python3.10/site-packages (from transformers) (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/lib/python3.10/site-packages (from transformers) (1.23.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/lucasvilsen/Library/Python/3.10/lib/python/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.10/site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/homebrew/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/lib/python3.10/site-packages (from requests->transformers) (1.26.12)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade huggingface_hub\n",
    "%pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForPreTraining\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForPreTraining\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Maltehb/danish-bert-botxo\")\n",
    "model = AutoModelForPreTraining.from_pretrained(\"Maltehb/danish-bert-botxo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [2, 2175, 82, 2567, 21100, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(\"hej jeg hedder lucas\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9758d6afbd64ae4974054fe7d7ceef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-15 12:28:29 INFO: Downloading default packages for language: da (Danish) ...\n",
      "2022-12-15 12:28:30 INFO: File exists: /Users/lucasvilsen/stanza_resources/da/default.zip\n",
      "2022-12-15 12:28:32 INFO: Finished downloading models and saved to /Users/lucasvilsen/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza.download(\"da\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 05:41:36 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07fd39821247432ca536c619338f4c6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 05:41:38 INFO: Loading these models for language: da (Danish):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | ddt       |\n",
      "| pos          | ddt       |\n",
      "| lemma        | ddt       |\n",
      "| depparse     | ddt       |\n",
      "| constituency | arboretum |\n",
      "| ner          | ddt       |\n",
      "============================\n",
      "\n",
      "2022-12-16 05:41:38 INFO: Use device: cpu\n",
      "2022-12-16 05:41:38 INFO: Loading: tokenize\n",
      "2022-12-16 05:41:38 INFO: Loading: pos\n",
      "2022-12-16 05:41:38 INFO: Loading: lemma\n",
      "2022-12-16 05:41:38 INFO: Loading: depparse\n",
      "2022-12-16 05:41:38 INFO: Loading: constituency\n",
      "2022-12-16 05:41:39 INFO: Loading: ner\n",
      "2022-12-16 05:41:39 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: i\tupos: ADP\txpos: None\tfeats: AdpType=Prep\n",
      "word: kan\tupos: AUX\txpos: None\tfeats: Mood=Ind|Tense=Pres|VerbForm=Fin|Voice=Act\n",
      "word: godt\tupos: ADV\txpos: None\tfeats: Degree=Pos\n",
      "word: lide\tupos: VERB\txpos: None\tfeats: VerbForm=Inf|Voice=Act\n",
      "word: at\tupos: PART\txpos: None\tfeats: PartType=Inf\n",
      "word: spiser\tupos: VERB\txpos: None\tfeats: Mood=Ind|Tense=Pres|VerbForm=Fin|Voice=Act\n",
      "word: mad\tupos: NOUN\txpos: None\tfeats: Definite=Ind|Gender=Com|Number=Sing\n",
      "word: fra\tupos: ADP\txpos: None\tfeats: AdpType=Prep\n",
      "word: carlsberg\tupos: PROPN\txpos: None\tfeats: _\n",
      "word: i\tupos: ADP\txpos: None\tfeats: AdpType=Prep\n",
      "word: københavn\tupos: PROPN\txpos: None\tfeats: _\n",
      "word: ,\tupos: PUNCT\txpos: None\tfeats: _\n",
      "word: danmark\tupos: PROPN\txpos: None\tfeats: _\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "pos_model = stanza.Pipeline(\"da\")\n",
    "text = \"i kan godt lide at spiser mad fra carlsberg i københavn, danmark\"\n",
    "result = pos_model(text)\n",
    "print(*[f'word: {word.text}\\tupos: {word.upos}\\txpos: {word.xpos}\\tfeats: {word.feats if word.feats else \"_\"}' for sent in result.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 'ADP', 'kan': 'AUX', 'godt': 'ADV', 'lide': 'VERB', 'at': 'PART', 'spiser': 'VERB', 'mad': 'NOUN', 'fra': 'ADP', 'carlsberg': 'PROPN', 'københavn': 'PROPN', ',': 'PUNCT', 'danmark': 'PROPN'}\n"
     ]
    }
   ],
   "source": [
    "dictionary = {}\n",
    "for sent in result.sentences:\n",
    "    for word in sent.words:\n",
    "        dictionary[word.text] = word.upos\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "my_lst = [\n",
    "['hej', 'Hej', 0, '\"hej\" skal begynde med stort bogstav: \"Hej\", da \"hej\" er det første ord i en ny sætning.'],\n",
    "['per', 'Per', 3, '\"per\" skal begynde med stort bogstav: \"Per\", da \"Per\" er et egenavn.'],\n",
    "['hedder', 'hedder,', 2, 'Der skal være komma efter \"hedder\"'],\n",
    "['Per', 'Per.', 3, 'Der skal være punktum efter \"Per\".'],\n",
    "['jeg', 'Jeg', 4, '\"jeg\" skal begynde med stort bogstav: \"Jeg\", da \"jeg\" er det første ord i en ny sætning.'],\n",
    "['carlsberg', 'Carlsberg', 12, '\"carlsberg\" skal begynde med stort bogstav: \"Carlsberg\", da \"Carlsberg\" er et egenavn.'],\n",
    "['københavn', 'København', 14, '\"københavn\" skal begynde med stort bogstav: \"København\", da \"København\" er et egenavn.'],\n",
    "['København', 'København.', 14, 'Der skal være punktum efter \"København\".']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hej', 'Hej', 0, '\"hej\" skal begynde med stort bogstav: \"Hej\", da \"hej\" er det første ord i en ny sætning.']\n",
      "['per', 'Per.', 3, '\"per\" skal begynde med stort bogstav: \"Per\", da \"Per\" er et egenavn. Der skal være punktum efter \"Per\".']\n",
      "['hedder', 'hedder,', 2, 'Der skal være komma efter \"hedder\"']\n",
      "['jeg', 'Jeg', 4, '\"jeg\" skal begynde med stort bogstav: \"Jeg\", da \"jeg\" er det første ord i en ny sætning.']\n",
      "['carlsberg', 'Carlsberg', 12, '\"carlsberg\" skal begynde med stort bogstav: \"Carlsberg\", da \"Carlsberg\" er et egenavn.']\n",
      "['københavn', 'København.', 14, '\"københavn\" skal begynde med stort bogstav: \"København\", da \"København\" er et egenavn. Der skal være punktum efter \"København\".']\n"
     ]
    }
   ],
   "source": [
    "def concat_duplicates(lst):\n",
    "    elements = {}\n",
    "    unique_lst = []\n",
    "    for sublist in lst:\n",
    "        if sublist[2] in elements.keys():\n",
    "            elements[sublist[2]][1] = sublist[1]\n",
    "            elements[sublist[2]][3] += \" \" + sublist[3]\n",
    "        else:\n",
    "            elements[sublist[2]] = sublist\n",
    "    return elements.values()\n",
    "\n",
    "print(*concat_duplicates(my_lst), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "pop index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [62], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m         lst\u001b[38;5;241m.\u001b[39mpop(value)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lst\n\u001b[0;32m---> 18\u001b[0m new_list \u001b[38;5;241m=\u001b[39m \u001b[43mconcat_duplicates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_lst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;241m*\u001b[39mnew_list, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn [62], line 15\u001b[0m, in \u001b[0;36mconcat_duplicates\u001b[0;34m(lst)\u001b[0m\n\u001b[1;32m     13\u001b[0m             to_pop\u001b[38;5;241m.\u001b[39mappend(j)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m to_pop:\n\u001b[0;32m---> 15\u001b[0m     \u001b[43mlst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lst\n",
      "\u001b[0;31mIndexError\u001b[0m: pop index out of range"
     ]
    }
   ],
   "source": [
    "def concat_duplicates(lst):\n",
    "    to_pop = []\n",
    "    for i in range(len(lst)):\n",
    "        for j in range(len(lst)):\n",
    "            if (i == j) or (i > j):\n",
    "                continue\n",
    "            if lst[i][2] == lst[j][2]:\n",
    "                wrong_word = lst[i][0]\n",
    "                right_word =lst[j][1]\n",
    "                index = lst[i][2]\n",
    "                description = lst[i][3] + \" \" + lst[j][3]\n",
    "                lst[i] = [wrong_word, right_word, index, description]\n",
    "                to_pop.append(j)\n",
    "    for value in to_pop:\n",
    "        lst.pop(value)\n",
    "    return lst\n",
    "\n",
    "new_list = concat_duplicates(my_lst)\n",
    "print(*new_list, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
